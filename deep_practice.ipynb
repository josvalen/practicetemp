{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOodsW5A6QGMeixQ2BJQtPW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josvalen/practicetemp/blob/how-do-i-do-this/deep_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "r7axwmfP5ofp"
      },
      "outputs": [],
      "source": [
        "# system level\n",
        "import sys\n",
        "import json\n",
        "import codecs\n",
        "import os\n",
        "\n",
        "# arrays\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "from scipy import interp\n",
        "\n",
        "# keras\n",
        "from keras import backend as K\n",
        "from keras import models\n",
        "from keras.models import Sequential, Model\n",
        "from keras.models import model_from_json\n",
        "from keras.layers import Input, Flatten, Dense, Activation, Dropout\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import Callback, ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# sklearn (for machine learning)\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import brier_score_loss\n",
        "\n",
        "# plotting\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from matplotlib.colors import LogNorm\n",
        "import seaborn as sns\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# grad-CAM\n",
        "## from vis.visualization import visualize_cam\n",
        "## from vis.utils import utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Input variables\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# training variables\n",
        "# 70:10:20 for training:valid:test\n",
        "nb_train = 10799\n",
        "nb_valid = 1542\n",
        "nb_test = 3085\n",
        "nb_epoch = 500\n",
        "nb_classes = 2\n",
        "batch_size = 128\n",
        "shuffle = True\n",
        "\n",
        "# data locations\n",
        "dir_test = \"model/\"\n",
        "# pristine images\n",
        "# for noisy images change SB00 to SB25 in all files that contain this part in their name\n",
        "file_x_data = \"SB00_augmented.npy\" # x data (images)\n",
        "file_y_data = \"SB00_augmented_y.npy\" # y data (labels)\n",
        "\n",
        "f_model = dir_test + \"SB00_model.json\" # model data (architecture)\n",
        "f_history = dir_test + \"SB00_history.json\" # training history\n",
        "f_weights = dir_test + \"SB00_weights.h5\" # model data (final weights)\n",
        "f_best_weights = dir_test + \"SB00_best_weights.h5\" # model data (best weights, give highest validation accuracy)\n",
        "\n",
        "merg_sub = \"subsamples_y.npy\" # file which contains past and future merger subsample labels\n",
        "subid_load = \"subsamples_ID.npy\" # file with all halo ID numbers in these subsamples, so we can keep track of the objects\n",
        "cc_load = \"concentration.npy\" # concentration\n",
        "m20_load = \"m20.npy\" # M20\n",
        "logMstar_load = \"logMstar.npy\" # log(M_star/M_10)"
      ],
      "metadata": {
        "id": "7vOAhxi16HfT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data, all filters\n",
        "x_data = np.load(file_x_data)\n",
        "y_data = np.load(file_y_data)\n",
        "\n",
        "# load pre and postmerger subsamples id halo IDs for these subsamples\n",
        "y_sub = np.load(merg_sub) # (merger-1/nonmerger-0, past merger-1/future merger-0, past merger-0/future merger-1)\n",
        "subids = np.load(subid_load)\n",
        "\n",
        "# load concentration, M20 and stellar mass LogM\n",
        "cc = np.load(cc_load)\n",
        "m20 = np.load(m20_load)\n",
        "logM = np.load(logMstar_load)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "bKfqHD1b8Oql",
        "outputId": "ec876d0f-9cc8-43fd-dc37-56e30097422f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-1ea85636b5e7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load data, all filters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_x_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_y_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# load pre and postmerger subsamples id halo IDs for these subsamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'SB00_augmented.npy'"
          ]
        }
      ]
    }
  ]
}